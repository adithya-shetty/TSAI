{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_word_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitdoda1983/EVA-Session-6/blob/master/6A_M6_Amit_Doda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eec1mAZGZG-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV1DPYhCy0Y_",
        "colab_type": "text"
      },
      "source": [
        "###Load IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_7V6d7hQmBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "imdb_dir = '/content/gdrive/My Drive/Colab Notebooks/data/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == '.txt':\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == 'neg':\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqUaEF6vS5aP",
        "colab_type": "text"
      },
      "source": [
        "###Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyGfms_1S-Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "maxlen = 100\n",
        "training_samples = 8000\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8ZE5y1oTWDY",
        "colab_type": "text"
      },
      "source": [
        "###Load Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WH3jhdeTadu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/gdrive/My Drive/Colab Notebooks/data', 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwgPc2rOUKM2",
        "colab_type": "text"
      },
      "source": [
        "###Define Model with Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMeMVbZ-Wcl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZ7inuWXb-5",
        "colab_type": "text"
      },
      "source": [
        "###Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nMM1BbFXfYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,epochs=10,batch_size=32,validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWcsO93uYCov",
        "colab_type": "text"
      },
      "source": [
        "###Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrInbo76YFXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv7iAO0OYMeF",
        "colab_type": "text"
      },
      "source": [
        "###Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ-ZpA4bYP1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(test_dir, label_type)\n",
        "  for fname in sorted(os.listdir(dir_name)):\n",
        "    if fname[-4:] == '.txt':\n",
        "    f = open(os.path.join(dir_name, fname))\n",
        "    texts.append(f.read())\n",
        "    f.close()\n",
        "    if label_type == 'neg':\n",
        "      labels.append(0)\n",
        "    else:\n",
        "      labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}